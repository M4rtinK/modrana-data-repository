## ** modRana data repository configuration file ** ##
##

## ** Folders ** ##

## folder for temporary data,p
## used during processing
temporary_folder=temp

## this folder will be initialized into a modRana
## data repository and packages will be copied to it
## once they are processed and packaged
repository_folder=results

## * Queue & Processing pool sizes * ##

## How repository update works
##
## (down)loader -> source queue -> processing pool ->
## -> packaging_queue -> packaging pool ->
## -> publishing queue -> publisher -> repository
##
## There is a single process that downloads source data packs,
## two multiprocessing pools that process and package
## (compress into a tarball) each data package and a publishing
## process that publishes the finished packages to the repository
## (as specified by the repository_folder variable).
##
##
## Estimating worst-case cpu core usage
##
## 1 (loader) + processing_pool_size + packaging_pool_size + 1 (publisher)
## This is really just the worst case scenario, as usually the loader and publisher
## will not be very CPU intensive and the packaging_pool will usually not be fully utilized,
## as package processing takes usually much longer than results packaging.
## Also, please note that individual packages might run their own processing in more than
## one thread - see the Monav section for more details.
## It is generally a good idea to overcommit on CPU usage a bit - just do some short test runs
## and see which settings result in the fastest update.
##
##
## Estimating RAM usage
##
## As the loader, publisher & packaging pool work incrementally with data from storage, they
## usually don't consume much RAM. The main RAM user is the processing pool. Generally, the less
## packages run in parallel in the processing pool, the less memory will the repository update need.
## NOTE: Big extracts, such as the France, Germany & Europe extracts might need up to 5-6 GB RAM
## just to process a single package.
##
##
## Estimating disk space during update
##
## Disk space usage during processing is not easy to estimate due to different
## package sizes. But you can limit it mainly through setting the source_queue_size and
## processing_pool_size variables, as packaging & publishing is usually quite fast, the packaging
## and publishing stages usually don't use much space. Once a package is publishes, it deletes all
## temporary data it used during processing.
## When doing a global update, it is advised to leave about 40-50 GB just for temporary data
## during processing.
##
##
## Repository size
##
## As of 24.10.2012 the Monav routing data repository needs about 91 GB disk space for global coverage.
##

## You can use this to override logical cpu count as returned by:
## python
## >>> import multiprocessing
## >>> multiprocessing
## multiprocessing.cpu_count()
##
#cpu_count=4

## Default value for all queues used during repository update
#queue_size=10

## How many packages can wait in the source data queue for processing
## DEFAULT: queue_size
##
#source_queue_size

## How many packages can wait in the packaging queue for packaging
## DEFAULT: queue_size
##
#packaging_queue_size=10

## How many packages can wait in the publishing queue for publishing
## DEFAULT: queue_size
##
#publish_queue_size

## How many packages can be processed in parallel
## DEFAULT: cpu_count
##
#processing_pool_size=4

## How many packages can be packaged in parallel
## DEFAULT: cpu_count
##
#packaging_pool_size=4

## * Monav * ##

## You can use the following variable to set path to the
## Monav preprocessor binary
#monav_preprocessor_path=/usr/bin/monav-preprocessor

## How many threads will each package in the processing pool use
## to run the monav-preprocessor
## NOTE: setting this to anything more than 3 currently doesn't
## add any more threads as there are only 3 runs needed for each package
## (car, bike, pedestrian)
## NOTE2: this basically multiplies per package memory usage by monav_parallel_threads,
## as the processing would be happening at once and not in sequence - take this in account
## when processing large extracts such as France, Germany or Europe - they can take 6+ GB RAM for
## SINGLE pass + there might be other large extracts being processed at the same time
## NOTE3: still, if you can afford this memory wise, you can get a <= 3x speedup
##
#monav_parallel_threads=3

## From what source PBF file size (in MB) should parallel Monav routing data processing
## NOT be used
## - like this, you can run processing of small packages in parallel but large packages
## will run their processing in sequence, lowering worst-case memory requirements
##
#monav_parallel_threshold=1000

## NOTE for monav_parallel_threads & monav_parallel_threshold:
## this does not govern how many package will be processed simultaneously, just internal Monav
## data package processing

## How many threads should the Monav-preprocessor run during the "elimination" phase
## DEFAULT: max(1,cpu_count/4)
## NOTE: total thread count for a SINGLE Monav data package package:
## monav_parallel_threads*monav_preprocessor_threads
## -> as only part of the work done by the monav-preprocessor is multi-threaded
## (that's why monav_parallel_threads exists), overcommiting on monav_preprocessor_threads
## is not that much of an issue
##
#monav_preprocessor_threads=4